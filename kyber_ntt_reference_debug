%%writefile kyber_ntt_reference_debug.cu
#include <stdio.h>
#include <stdint.h>
#include <cuda_runtime.h>
#include <stdlib.h>

#define N 256
#define Q 3329
#define F 1441

__device__ __constant__ int16_t d_zetas[128];

const int16_t h_zetas[128] = {
  -1044,-758,-359,-1517,1493,1422,287,202,
  -171,622,1577,182,962,-1202,-1474,1468,
  573,-1325,264,383,-829,1458,-1602,-130,
  -681,1017,732,608,-1542,411,-205,-1571,
  1223,652,-552,1015,-1293,1491,-282,-1544,
  516,-8,-320,-666,-1618,-1162,126,1469,
  -853,-90,-271,830,107,-1421,-247,-951,
  -398,961,-1508,-725,448,-1065,677,-1275,
  -1103,430,555,843,-1251,871,1550,105,
  422,587,177,-235,-291,-460,1574,1653,
  -246,778,1159,-147,-777,1483,-602,1119,
  -1590,644,-872,349,418,329,-156,-75,
  817,1097,603,610,1322,-1285,-1465,384,
  -1215,-136,1218,-1335,-874,220,-1187,-1659,
  -1185,-1530,-1278,794,-1510,-854,-870,478,
  -108,-308,996,991,958,-1460,1522,1628
};
// =================================================
// GPU helper: fqmul
// MUST be visible before kernels
// =================================================
__device__ __forceinline__ int16_t fqmul_gpu(int16_t a, int16_t b) {
    int32_t r = (int32_t)a * b;
    r %= Q;
    if (r < 0) r += Q;
    return (int16_t)r;
}

static inline int16_t fqmul_cpu(int16_t a, int16_t b) {
    int32_t r = (int32_t)a * b;
    r %= Q;
    if (r < 0) r += Q;
    return (int16_t)r;
}
void cpu_ntt(int16_t *a) {
    int k = 1;
    for (int len = 128; len >= 2; len >>= 1) {
        for (int start = 0; start < 256; start += 2 * len) {
            int16_t zeta = h_zetas[k++];
            for (int j = start; j < start + len; j++) {
                int16_t t = fqmul_cpu(zeta, a[j + len]);
                int16_t u = a[j];
                a[j] = u + t; if (a[j] >= Q) a[j] -= Q;
                a[j + len] = u - t; if (a[j + len] < 0) a[j + len] += Q;
            }
        }
    }
}
void cpu_invntt(int16_t *a) {
    int k = 127;
    for (int len = 2; len <= 128; len <<= 1) {
        for (int start = 0; start < 256; start += 2 * len) {
            int16_t zeta = h_zetas[k--];
            for (int j = start; j < start + len; j++) {
                int16_t t = a[j];
                a[j] = (t + a[j + len]) % Q;
                a[j + len] = fqmul_cpu(zeta, a[j + len] - t);
            }
        }
    }
    for (int i = 0; i < 256; i++)
        a[i] = fqmul_cpu(a[i], F);
}
__global__ void kyber_ntt_batched(int16_t *a) {
    int poly = blockIdx.x;
    int tid  = threadIdx.x;

    __shared__ int16_t s[N];
    int base = poly * N;

    s[tid]       = a[base + tid];
    s[tid + 128] = a[base + tid + 128];
    __syncthreads();

    int k = 1;

    for (int len = 128; len >= 2; len >>= 1) {
        for (int start = 0; start < N; start += 2 * len) {
            int16_t zeta = d_zetas[k++];

            if (tid < len) {
                int i = start + tid;
                int j = i + len;

                int16_t t = fqmul_gpu(zeta, s[j]);
                int16_t u = s[i];

                s[i] = u + t; if (s[i] >= Q) s[i] -= Q;
                s[j] = u - t; if (s[j] < 0) s[j] += Q;
            }
            __syncthreads();
        }
    }

    a[base + tid]       = s[tid];
    a[base + tid + 128] = s[tid + 128];
}
__global__ void kyber_pointwise_batched(
    const int16_t *A,
    const int16_t *B,
    int16_t *C
) {
    int poly = blockIdx.x;
    int tid  = threadIdx.x;

    int idx = poly * N + tid;
    C[idx] = fqmul_gpu(A[idx], B[idx]);
}
__global__ void kyber_invntt_batched(int16_t *a) {
    int poly = blockIdx.x;
    int tid  = threadIdx.x;

    __shared__ int16_t s[N];
    int base = poly * N;

    s[tid]       = a[base + tid];
    s[tid + 128] = a[base + tid + 128];
    __syncthreads();

    int k = 127;

    for (int len = 2; len <= 128; len <<= 1) {
        for (int start = 0; start < N; start += 2 * len) {
            int16_t zeta = d_zetas[k--];

            if (tid < len) {
                int i = start + tid;
                int j = i + len;

                int16_t t = s[i];
                int16_t u = s[j];

                s[i] = (t + u) % Q;
                s[j] = fqmul_gpu(zeta, u - t);
            }
            __syncthreads();
        }
    }

    s[tid]       = fqmul_gpu(s[tid], F);
    s[tid + 128] = fqmul_gpu(s[tid + 128], F);
    __syncthreads();

    a[base + tid]       = s[tid];
    a[base + tid + 128] = s[tid + 128];
}
void compare(const char *tag, int16_t *gpu, int16_t *cpu) {
    for (int i = 0; i < N; i++) {
        if (gpu[i] != cpu[i]) {
            printf("[%s ERROR] index %d gpu=%d cpu=%d\n",
                   tag, i, gpu[i], cpu[i]);
            exit(0);
        }
    }
    printf("[OK] %s matches\n", tag);
}
int main() {
    cudaMemcpyToSymbol(d_zetas, h_zetas, sizeof(h_zetas));

    int16_t hA[N], hB[N];
    int16_t cpuA[N], cpuB[N];

    for (int i = 0; i < N; i++) {
        hA[i] = cpuA[i] = rand() % Q;
        hB[i] = cpuB[i] = rand() % Q;
    }

    int16_t *dA, *dB;
    cudaMalloc(&dA, N * sizeof(int16_t));
    cudaMalloc(&dB, N * sizeof(int16_t));

    cudaMemcpy(dA, hA, N * sizeof(int16_t), cudaMemcpyHostToDevice);
    cudaMemcpy(dB, hB, N * sizeof(int16_t), cudaMemcpyHostToDevice);

    /* ---------- NTT ---------- */
    cpu_ntt(cpuA);
    kyber_ntt_batched<<<1,128>>>(dA);
    cudaMemcpy(hA, dA, N * sizeof(int16_t), cudaMemcpyDeviceToHost);
    compare("NTT", hA, cpuA);

    /* ---------- POINTWISE ---------- */
    for (int i = 0; i < N; i++)
        cpuA[i] = fqmul_cpu(cpuA[i], cpuB[i]);

    kyber_pointwise_batched<<<1,256>>>(dA, dB, dA);
    cudaMemcpy(hA, dA, N * sizeof(int16_t), cudaMemcpyDeviceToHost);
    compare("POINTWISE", hA, cpuA);

    /* ---------- INV NTT ---------- */
    cpu_invntt(cpuA);
    kyber_invntt_batched<<<1,128>>>(dA);
    cudaMemcpy(hA, dA, N * sizeof(int16_t), cudaMemcpyDeviceToHost);
    compare("INVNTT", hA, cpuA);

    printf("\nâœ… ALL STEPS VERIFIED SUCCESSFULLY\n");
    return 0;
}
