%%writefile kyber_gpu_verify.cu
#include <stdio.h>
#include <stdint.h>
#include <cuda_runtime.h>

#define N 256
#define Q 3329
#define F 1441

// =====================================================
// Kyber zetas (reference order)
// =====================================================
__device__ __constant__ int16_t d_zetas[128];

const int16_t h_zetas[128] = {
 -1044,-758,-359,-1517,1493,1422,287,202,
 -171,622,1577,182,962,-1202,-1474,1468,
 573,-1325,264,383,-829,1458,-1602,-130,
 -681,1017,732,608,-1542,411,-205,-1571,
 1223,652,-552,1015,-1293,1491,-282,-1544,
 516,-8,-320,-666,-1618,-1162,126,1469,
 -853,-90,-271,830,107,-1421,-247,-951,
 -398,961,-1508,-725,448,-1065,677,-1275,
 -1103,430,555,843,-1251,871,1550,105,
 422,587,177,-235,-291,-460,1574,1653,
 -246,778,1159,-147,-777,1483,-602,1119,
 -1590,644,-872,349,418,329,-156,-75,
 817,1097,603,610,1322,-1285,-1465,384,
 -1215,-136,1218,-1335,-874,220,-1187,-1659,
 -1185,-1530,-1278,794,-1510,-854,-870,478,
 -108,-308,996,991,958,-1460,1522,1628
};
// --------------------------------------------------
// CPU helper: Kyber modular multiply
// --------------------------------------------------
static inline int16_t fqmul_cpu(int16_t a, int16_t b) {
    int32_t r = (int32_t)a * b;
    r %= Q;
    if (r < 0) r += Q;
    return (int16_t)r;
}

// =====================================================
// Helpers
// =====================================================
__device__ __forceinline__ int16_t fqmul(int16_t a, int16_t b) {
    int32_t r = (int32_t)a * b;
    r %= Q;
    if (r < 0) r += Q;
    return (int16_t)r;
}

// =====================================================
// GPU kernels (one block, 128 threads, one poly)
// =====================================================
__global__ void ntt_kernel(int16_t *a) {
    __shared__ int16_t s[N];
    int tid = threadIdx.x;

    s[tid]       = a[tid];
    s[tid + 128] = a[tid + 128];
    __syncthreads();

    int k = 1;
    for (int len = 128; len >= 2; len >>= 1) {
        for (int start = 0; start < N; start += 2 * len) {
            int16_t zeta = d_zetas[k++];
            if (tid < len) {
                int i = start + tid;
                int j = i + len;
                int16_t t = fqmul(zeta, s[j]);
                int16_t u = s[i];
                s[i] = u + t; if (s[i] >= Q) s[i] -= Q;
                s[j] = u - t; if (s[j] < 0) s[j] += Q;
            }
            __syncthreads();
        }
    }

    a[tid]       = s[tid];
    a[tid + 128] = s[tid + 128];
}

__global__ void invntt_kernel(int16_t *a) {
    __shared__ int16_t s[N];
    int tid = threadIdx.x;

    s[tid]       = a[tid];
    s[tid + 128] = a[tid + 128];
    __syncthreads();

    int k = 127;
    for (int len = 2; len <= 128; len <<= 1) {
        for (int start = 0; start < N; start += 2 * len) {
            int16_t zeta = d_zetas[k--];
            if (tid < len) {
                int i = start + tid;
                int j = i + len;
                int16_t t = s[i];
                s[i] = (t + s[j]) % Q;
                s[j] = fqmul(zeta, s[j] - t);
            }
            __syncthreads();
        }
    }

    s[tid]       = fqmul(s[tid], F);
    s[tid + 128] = fqmul(s[tid + 128], F);
    __syncthreads();

    a[tid]       = s[tid];
    a[tid + 128] = s[tid + 128];
}

__global__ void pointwise_kernel(
    const int16_t *a,
    const int16_t *b,
    int16_t *c
) {
    int tid = threadIdx.x;
    c[tid] = fqmul(a[tid], b[tid]);
}

// =====================================================
// HOST-CALLABLE GPU FUNCTIONS (THIS IS WHAT YOU WANT)
// =====================================================
extern "C" {

void kyber_gpu_ntt(int16_t *a) {
    int16_t *d;
    cudaMalloc(&d, N * sizeof(int16_t));
    cudaMemcpy(d, a, N * sizeof(int16_t), cudaMemcpyHostToDevice);

    ntt_kernel<<<1,128>>>(d);
    cudaDeviceSynchronize();

    cudaMemcpy(a, d, N * sizeof(int16_t), cudaMemcpyDeviceToHost);
    cudaFree(d);
}

void kyber_gpu_invntt(int16_t *a) {
    int16_t *d;
    cudaMalloc(&d, N * sizeof(int16_t));
    cudaMemcpy(d, a, N * sizeof(int16_t), cudaMemcpyHostToDevice);

    invntt_kernel<<<1,128>>>(d);
    cudaDeviceSynchronize();

    cudaMemcpy(a, d, N * sizeof(int16_t), cudaMemcpyDeviceToHost);
    cudaFree(d);
}

void kyber_gpu_pointwise(
    const int16_t *a,
    const int16_t *b,
    int16_t *c
) {
    int16_t *da, *db, *dc;
    cudaMalloc(&da, N * sizeof(int16_t));
    cudaMalloc(&db, N * sizeof(int16_t));
    cudaMalloc(&dc, N * sizeof(int16_t));

    cudaMemcpy(da, a, N * sizeof(int16_t), cudaMemcpyHostToDevice);
    cudaMemcpy(db, b, N * sizeof(int16_t), cudaMemcpyHostToDevice);

    pointwise_kernel<<<1,256>>>(da, db, dc);
    cudaDeviceSynchronize();

    cudaMemcpy(c, dc, N * sizeof(int16_t), cudaMemcpyDeviceToHost);

    cudaFree(da); cudaFree(db); cudaFree(dc);
}

}
// =====================================================
// TEST MAIN (for verification only)
// =====================================================
int main() {
    cudaMemcpyToSymbol(d_zetas, h_zetas, sizeof(h_zetas));

    int16_t a[N], b[N], c_gpu[N], c_cpu[N];

    for (int i = 0; i < N; i++) {
        a[i] = i % Q;
        b[i] = (i * 7) % Q;
    }

    // ---------------- GPU pipeline ----------------
    int16_t a_gpu[N], b_gpu[N];
    for (int i = 0; i < N; i++) {
        a_gpu[i] = a[i];
        b_gpu[i] = b[i];
    }

    kyber_gpu_ntt(a_gpu);
    kyber_gpu_ntt(b_gpu);
    kyber_gpu_pointwise(a_gpu, b_gpu, c_gpu);
    kyber_gpu_invntt(c_gpu);

    // ---------------- CPU reference ----------------
    int16_t A[N], B[N];
    for (int i = 0; i < N; i++) {
        A[i] = a[i];
        B[i] = b[i];
    }

    // CPU NTT
    int k = 1;
    for (int len = 128; len >= 2; len >>= 1) {
        for (int start = 0; start < N; start += 2 * len) {
            int16_t zeta = h_zetas[k++];
            for (int j = start; j < start + len; j++) {
                int16_t t = fqmul_cpu(zeta, A[j + len]);
                int16_t u = A[j];
                A[j] = u + t; if (A[j] >= Q) A[j] -= Q;
                A[j + len] = u - t; if (A[j + len] < 0) A[j + len] += Q;
            }
        }
    }

    k = 1;
    for (int len = 128; len >= 2; len >>= 1) {
        for (int start = 0; start < N; start += 2 * len) {
            int16_t zeta = h_zetas[k++];
            for (int j = start; j < start + len; j++) {
                int16_t t = fqmul_cpu(zeta, B[j + len]);
                int16_t u = B[j];
                B[j] = u + t; if (B[j] >= Q) B[j] -= Q;
                B[j + len] = u - t; if (B[j + len] < 0) B[j + len] += Q;
            }
        }
    }

    // CPU pointwise
    for (int i = 0; i < N; i++)
        A[i] = fqmul_cpu(A[i], B[i]);

    // CPU inverse NTT
    k = 127;
    for (int len = 2; len <= 128; len <<= 1) {
        for (int start = 0; start < N; start += 2 * len) {
            int16_t zeta = h_zetas[k--];
            for (int j = start; j < start + len; j++) {
                int16_t t = A[j];
                A[j] = (t + A[j + len]) % Q;
                A[j + len] = fqmul_cpu(zeta, A[j + len] - t);
            }
        }
    }

    for (int i = 0; i < N; i++)
        c_cpu[i] = fqmul_cpu(A[i], 1441);

    // ---------------- Compare ----------------
    for (int i = 0; i < N; i++) {
        if (c_gpu[i] != c_cpu[i]) {
            printf("❌ Mismatch at %d: gpu=%d cpu=%d\n", i, c_gpu[i], c_cpu[i]);
            return 1;
        }
    }

    printf("✅ GPU NTT + POINTWISE + INVNTT VERIFIED\n");
    return 0;
}
