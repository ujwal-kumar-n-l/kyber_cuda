%%writefile kyber_invntt_batched.cu
#include <stdio.h>
#include <stdint.h>
#include <cuda_runtime.h>
#include <stdlib.h>

#define N 256
#define Q 3329
#define F 1441
#ifndef BATCH
#define BATCH 4096
#endif

// --------------------------------------------------
// Kyber reference zetas
// --------------------------------------------------
__device__ __constant__ int16_t d_zetas[128];

const int16_t h_zetas[128] = {
  -1044,  -758,  -359, -1517,  1493,  1422,   287,   202,
   -171,   622,  1577,   182,   962, -1202, -1474,  1468,
    573, -1325,   264,   383,  -829,  1458, -1602,  -130,
   -681,  1017,   732,   608, -1542,   411,  -205, -1571,
   1223,   652,  -552,  1015, -1293,  1491,  -282, -1544,
    516,    -8,  -320,  -666, -1618, -1162,   126,  1469,
   -853,   -90,  -271,   830,   107, -1421,  -247,  -951,
   -398,   961, -1508,  -725,   448, -1065,   677, -1275,
  -1103,   430,   555,   843, -1251,   871,  1550,   105,
    422,   587,   177,  -235,  -291,  -460,  1574,  1653,
   -246,   778,  1159,  -147,  -777,  1483,  -602,  1119,
  -1590,   644,  -872,   349,   418,   329,  -156,   -75,
    817,  1097,   603,   610,  1322, -1285, -1465,   384,
  -1215,  -136,  1218, -1335,  -874,   220, -1187, -1659,
  -1185, -1530, -1278,   794, -1510,  -854,  -870,   478,
   -108,  -308,   996,   991,   958, -1460,  1522,  1628
};

// --------------------------------------------------
// CPU helper
// --------------------------------------------------
static inline int16_t fqmul_cpu(int16_t a, int16_t b) {
    int32_t r = (int32_t)a * b;
    r %= Q;
    if (r < 0) r += Q;
    return (int16_t)r;
}

// --------------------------------------------------
// GPU helper
// --------------------------------------------------
__device__ __forceinline__ int16_t fqmul_gpu(int16_t a, int16_t b) {
    int32_t r = (int32_t)a * b;
    r %= Q;
    if (r < 0) r += Q;
    return (int16_t)r;
}

// --------------------------------------------------
// Batched inverse NTT kernel (GPU)
// --------------------------------------------------
__global__ void kyber_invntt_batched(int16_t *d_all) {
    int poly = blockIdx.x;
    int tid  = threadIdx.x;

    __shared__ int16_t s[N];

    int base = poly * N;

    // load polynomial
    s[tid]       = d_all[base + tid];
    s[tid + 128] = d_all[base + tid + 128];
    __syncthreads();

    int k_base = 127;

    for (int len = 2; len <= 128; len <<= 1) {
        int blocks = N / (2 * len);

        if (tid < len) {
            for (int sb = 0; sb < blocks; sb++) {
                int idx1 = sb * (2 * len) + tid;
                int idx2 = idx1 + len;

                int16_t zeta = d_zetas[k_base - sb];
                int16_t t    = s[idx1];
                int16_t u    = s[idx2];

                s[idx1] = (t + u) % Q;
                s[idx2] = fqmul_gpu(zeta, u - t);
            }
        }

        k_base -= blocks;
        __syncthreads();
    }

    // final scaling
    s[tid]       = fqmul_gpu(s[tid], F);
    s[tid + 128] = fqmul_gpu(s[tid + 128], F);
    __syncthreads();

    // write back
    d_all[base + tid]       = s[tid];
    d_all[base + tid + 128] = s[tid + 128];
}


// --------------------------------------------------
// CPU reference inverse NTT
// --------------------------------------------------
void kyber_invntt_cpu(int16_t *r) {
    int k = 127;
    for (int len = 2; len <= 128; len <<= 1) {
        for (int start = 0; start < 256; start += 2 * len) {
            int16_t zeta = h_zetas[k--];
            for (int j = start; j < start + len; j++) {
                int16_t t = r[j];
                r[j] = (t + r[j + len]) % Q;
                r[j + len] = fqmul_cpu(zeta, r[j + len] - t);
            }
        }
    }
    for (int j = 0; j < 256; j++)
        r[j] = fqmul_cpu(r[j], F);
}

// --------------------------------------------------
// MAIN
// --------------------------------------------------
int main() {
    printf("Running batched inverse NTT (BATCH=%d)\n", BATCH);

    cudaMemcpyToSymbol(d_zetas, h_zetas, sizeof(h_zetas));

    size_t bytes = (size_t)BATCH * N * sizeof(int16_t);
    int16_t *h = (int16_t*)malloc(bytes);
    for (int i = 0; i < BATCH * N; i++) h[i] = i % Q;

    int16_t *d;
    cudaMalloc(&d, bytes);
    cudaMemcpy(d, h, bytes, cudaMemcpyHostToDevice);

    kyber_invntt_batched<<<BATCH, 128>>>(d);
    cudaDeviceSynchronize();

    int16_t ref[N];
    for (int i = 0; i < N; i++) ref[i] = i % Q;
    kyber_invntt_cpu(ref);

    cudaMemcpy(h, d, N * sizeof(int16_t), cudaMemcpyDeviceToHost);

    int mismatch = 0;
    for (int i = 0; i < N; i++) {
        if (h[i] != ref[i]) {
            printf("Mismatch at %d: gpu=%d cpu=%d\n", i, h[i], ref[i]);
            mismatch = 1;
            break;
        }
    }

    printf("Correctness mismatch = %d\n", mismatch);

    cudaFree(d);
    free(h);
    return 0;
}
